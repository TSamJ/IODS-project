---
title: "Assignment 6"
author: "Tuomas Junna"
date: '2023-12-08'
output: html_document
---

# Assignment 6

## Analysis of longitudinal data

This is the sixt assignment of the IODS course, focusing on longitudinal data. At the core of this assignment is the difference between "wide" and "long" data referring  to different ways of organizing and structuring data. The choice between wide and long format depends on the analysis and visualization tasks one intends to perform. We will explore this further with examples of our assignment data, as per instructed in the data wrangling assignment. 

Initially, we load in the data and explore it using the meet_and_repeat.R script produced in the data wrangling exercise.

```{r}
# Author: Tuomas Junna
#Date: 08.12.2023
# Data wrangling R script for IODS course assignment 6. 
#Origina data source : # Original data from: http://hdr.undp.org/en/content/human-development-index-hdi


library(readr)
library(tidyverse)
library(dplyr)
library(ggplot2)

#Set working directory

setwd("C:/Users/tjunna/OneDrive - Valtori GTK/Desktop/R/IODS/IODS-project")

#Read datasets

RATS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/rats.txt", header = TRUE, sep = '\t')

BPRS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/BPRS.txt", sep  =" ", header = T)

# Factor treatment & subject BPRS
BPRS$treatment <- factor(BPRS$treatment)
BPRS$subject <- factor(BPRS$subject)

# Convert to long form BPRS
BPRSL <-  pivot_longer(BPRS, cols = -c(treatment, subject),
                       names_to = "weeks", values_to = "bprs") %>%
  arrange(weeks) #order by weeks variable

# Extract the week number
BPRSL <-  BPRSL %>% 
  mutate(week = as.integer(substr(weeks, 5,5)))

# Factor variables ID and Group in RATS

RATS$ID <- factor(RATS$ID)
RATS$Group <- factor(RATS$Group)

# Convert data to long form
RATSL <- pivot_longer(RATS, cols = -c(ID, Group), 
                      names_to = "WD",
                      values_to = "Weight") %>% 
  mutate(Time = as.integer(substr(WD,3,4))) %>%
  arrange(Time)

# Glimpse the data
glimpse(RATSL)

glimpse(BPRSL)

#Explore dataframes

str(RATS)
dim(RATS)
names(RATS)

str(RATSL)
dim(RATSL)
names(RATSL)

```
**Data exploration**

Now if we explore the dimensions dataframe RATS and it's longform version of RATSL, we notice that the RATS table contains effectively headers on top, 16 rows and 13 columns, or in R speak, 16 obserations of 13 variables. The essential thing to notice is that **each variable has its own column and each row represents an observation or a unit of analysis.**

This is the wide format of data and is the one many are most familiar with due to it being the standard spreadsheet appearance. 

Now the dimension of RATSL is 176 observations of 5 variables. The key difference being that there can be **multiple rows for each observation and that variables and values are stored in two columns: one for variable names and another for values.**

While the wide approach is the one generally used in data-entry and summary statistics due to it's more approachable and intuitive layout, dealing with the PITA that is long form data is often necessary for more complex analyses, especially when dealing with mixed-effects models or repeated measures.

In this assignment, no comprehensive meta data exploration was done/ or available, so suffice to say that apparently the RATS dataframe deals with lab rats weight changes over time and the BRSP data frame deals with human response to treatment. Both datasets are similar in the sense that they are broken to  groups. In BPRS there are two groups with presumably two different treatments allowing the comparison between the two. The rats are separated in three groups, and we can assume that perhaps differentiated by diet or other treatment.

We will start exploring RATS(long) first. 


```{r}

# Draw the plot
ggplot(RATSL, aes(x = Time, y = Weight, linetype = ID)) +
  geom_line() +
  #scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ Group, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = c(min(RATSL$Weight), max(RATSL$Weight)))

```



Here we can see that in our three groups of rats, rats belonging to group 1 are considerably smaller and do not show much weight gain over time. The chonkier rats in groups 2 and 3 show faster weight gain that plateaus or turns to weight loss around the 40 "time unit" mark and then kicks off with a noticeable upturn. In other words, the rats higher initial weight seem to gain more weight as time goes on. Is this an example of **tracking?** 

It's standardization time then, via the standard error of mean

$$se = \frac{sd(x)}{\sqrt{n}}$$

```{r}


RATSL <- RATSL %>%
  group_by(Time) %>%
  mutate( stdweight = (Weight - mean(Weight))/sd(Weight) ) %>%
  ungroup()

# Draw the plot
ggplot(RATSL, aes(x = Time, y = stdweight, linetype = ID)) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ Group, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = c(min(RATSL$stdweight), max(RATSL$stdweight)))

```
Standardized values per group over time. This seems wildly unuseful if we assume we are studying diets.



```{r}

# Summary data with mean and standard error of bprs by treatment and week 
RATSS <- RATSL %>%
  group_by(Group, Time) %>%
  summarise( mean = mean(Weight), se = sd(Weight)) %>%
  ungroup()

# Plot the mean profiles
library(ggplot2)
ggplot(RATSS, aes(x = Time, y = mean, linetype = Group, shape = Group)) +
  geom_line() +
  scale_linetype_manual(values = c(1,2,3)) +
  geom_point(size=3) +
  scale_shape_manual(values = c(1,2,3)) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se, linetype="1"), width=0.3) +
  theme(legend.position = c(0.9,0.35)) +
  scale_y_continuous(name = "mean(Weight) +/- se(Weight)")
```
Mean weight gain per group over time. 

Well, yes. Earlier remarks about the general trends seem to hold true. 

##Summary Measure Analysis of Longitudinal Data##

For the summary measure analysis, careful consideration of the chosen measure should be taken. Given that we have absolutely no idea or real interest about the data or the scientific context of it, the selection will be completely arbitrary. As we just need chonky rats in our life, regression coefficient describing rates of weight gain and just overall mean growth seem like the most important ones for cute fat rat overload. The motivation of this study is described in an image below: 

![Chonker of a rat](fatrat.jpg)
<br>
What an absolute unit.

However, for the sake of exercise, we will use only data from after 42 time units as our summary measure. Because it's the answer to everything and also because it's around the time when weight gain intensifies in groups 2 and 3.
The data is checked for outliers: 


```{r}
# Create a summary data by treatment and subject with mean as the summary variable (ignoring baseline week 0)
RATSL42 <- RATSL %>%
  filter(Time >= 42) %>%
  group_by(Group, Time) %>%
  summarise( mean=mean(Weight) ) %>%
  ungroup()

# Glimpse the data
glimpse(RATSL42)

# Draw a boxplot of the mean versus treatment
library(ggplot2)
ggplot(RATSL42, aes(x = Group, y = mean)) +
  geom_boxplot() +
  stat_summary(fun = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "mean(Weight), Time 42 onwards")

# Create a new data by filtering the outlier and adjust the ggplot code the draw the plot again with the new data
#BPRSL8S1 <- BPRSL8S %>%
 # filter(mean < 60)
```
There appear to be no outliers that need to be removed.  We will filter out a single rat, named The King Chonk as it has reached a status of max chonk and thus is out of reach of mere human sciences. This graph also confirms that our suspicions, that rats in group 1 are the smallest and the rats in group 3 are the largest. But let's not rely on such unreliable visual cues as this graph and let's do some tests using the WD1 times as a baseline.

As we have have decided that we are interested particularly in the growth of group 1 and t-test are done to pairs, we will compare groups 1 and 2 against each other as well as groups 1 and 3.

We will also run a threeway t-test (oneway test)

** T for test and A for Anova**

```{r}

# Create a new data by filtering the outlier and adjust the ggplot code the draw the plot again with the new data
RATSL422 <- RATSL42 %>%
  filter(mean < 545)


# Extract data for each group
group1_data <- RATSL422$mean[RATSL422$Group == 1]
group2_data <- RATSL422$mean[RATSL422$Group == 2]
group3_data <- RATSL422$mean[RATSL422$Group == 3]

# Perform three-sample t-test
t_test_result <- oneway.test(mean ~ Group, data = RATSL422, var.equal = TRUE)

# Print the results
print(t_test_result)

# Perform a two-sample t-tests

t_test_result12 <- t.test(group1_data, group2_data)
t_test_result13 <- t.test(group1_data, group3_data)

#print

print(t_test_result12)
print(t_test_result13)


# Add the baseline from the original data as a new variable to the summary data
rotta <- RATSL42 %>%
  mutate(baseline = mean(RATS$WD1))

# Fit the linear model with the mean as the response 
fit <- lm(mean ~baseline + Group, data = rotta)

# Compute the analysis of variance table for the fitted model with anova()

anova_result <- anova(fit)
print(anova_result)

```

First in the output we have the one-way analysis results:

F-statistic: 1085.6
Degrees of freedom (numerator): 2
Degrees of freedom (denominator): 11
P-value: 2.311e-13.


So as we have a very low p-value, we can figure out that that at least one of the explanatory variables, baseline or group, has a statistically significant effect on the mean weight of rats. As we also know, that the rats in group 1 are much smaller, we might go as far as to say that that this all stems from the fact that rats in group 1 weight less.


Looking at the paired group comparisons, we get the two sample t test between groups 1 and 2 with values:

t-statistic: -37.427
Degrees of freedom: 4.3382
P-value: 1.299e-06
Alternative hypothesis: The true difference in means is not equal to 0
95% confidence interval: (-247.5414, -214.3086)
Sample estimates:
Mean of group1_data: 269.875
Mean of group2_data: 500.800

As we can not spot that the mean weight of group 2 rats is almost twice as the mean weight of group 1, the low p-value (1.299e-06) suggests that there is... Indeed a significant difference in means between the two groups. 95 % confidence interval range gives range within which we can be reasonably confident the true difference in means lies between mentioned values. 
And it's pretty much the same story for comparison of groups 1 and 3.

Analysis of Variance output in a bit wider form is:


Between Groups (Group):
Degrees of Freedom (Df): 2
Sum of Squares (Sum Sq): 209735
Mean Square (Mean Sq): 104868
F-value: 1031.1
P-value (Pr(>F)): 3.75e-14 (very low, indicating significance)
Within Groups (Residuals):
Degrees of Freedom (Df): 12
Sum of Squares (Sum Sq): 1220
Mean Square (Mean Sq): 102


This is to say that the overall model is significant (p-value < 0.05), suggesting that at least one of the groups has a different mean. The significant F-value (1031.1) indicates that the variation between group means is larger than what would be expected by random chance. The significant p-value (3.75e-14) provides evidence against the null hypothesis of equal means.

The "Signif. codes" section indicates the level of significance where *** means very significant (p-value < 0.001), ** means significant (p-value < 0.01), and * means marginally significant (p-value < 0.05).

Overall, these results suggest that there are statistically significant differences in the mean weights among the groups of rats. 


Ok, now that we have proven to some degree of certainty that small rats weight less large rats, let's move on to a much less interesting topic: humans and their treatment outcomes. 

![itiswhatitis](sick.jpg)
```{r}

# Plot the data

ggplot(BPRSL, aes(x = week, y = bprs, linetype = subject, color = treatment)) +
  geom_line() +
  labs(color = "Treatment") +
  guides(color = guide_legend(title = "Treatment group")) 



```

So pivoted to long data form, this dataset contains 360 observations of 5 variables. It has people divided into two groups and they have been studied over a period of eight weeks. In the plot, on the x axis is time in weeks and on Y axis is some index or indicator of health, where lower is probably better. It could refer to "Brief Psychiatric Rating Scale" so that these would be more psychological symptom response to treatment. 

We will now create a regression model with "bprs" as the response variable and time and treatment as explanatory variables, in other words, see how the (mental?) health indicator changes in response to time between the two different treatment groups. 

```{r}


# create a regression model for rat race burnouts
RATRACE <- lm(bprs ~ week + treatment, data = BPRSL)

# print out a summary of the model

summary(RATRACE)

```

So interpret the key aspects of model summary output: 

Statistical Significance:

Both the intercept and the slope for 'week' are statistically significant (p-value < 0.001), indicating a significant linear relationship.

The slope for 'treatment2' is not statistically significant (p-value = 0.661), suggesting that this variable may not be contributing significantly to explaining the variability in 'bprs.'

The R-squared value is 0.1851, indicating that the model explains 18.51% of the variance in the dependent variable.

The F-statistic tests the overall significance of the model. A high F-statistic (40.55) with a very low p-value (< 2.2e-16) suggests that the overall model is significant.

This output suggests that time has a significant reductive effect on mental health indicators while treatment group does not seem to have a significant effect in the context of this model. 

**Random intercept and random slope model**

Now we can move on to fit the random intercept and random slope model allowing us to differentiate between different subjects.

```{r}

# create a random intercept and random slope model
library(lme4)



psych_ref <- lmer(bprs ~ week + treatment + (1 | subject), data = BPRSL, REML = FALSE)

psych_ref1 <- lmer(bprs ~ week + treatment + (week | subject), data = BPRSL, REML = FALSE)

# print a summary of the model

summary(psych_ref1)

# perform an ANOVA test on the two models
anova(psych_ref1, psych_ref)

```
The response variable is "bprs," and the model includes fixed effects for "week" and "treatment."
The random effects structure includes random intercepts and slopes for "week" within the grouping variable "subject" ie. individual test subjects This means that each subject may have a different intercept and slope for the effect of time on their mental health indicators.

So let's interpret the output: 

AIC (Akaike Information Criterion): A measure of model fit that penalizes complex models. Lower AIC values indicate better-fitting models. We have close to three thousand and I suppose that isn't the smallest imaginable number.

BIC (Bayesian Information Criterion): Similar to AIC but penalizes models more for complexity. 

Log Likelihood: The log-likelihood of the model, a measure of how well the model explains the observed data. So this is in the range of -1400. Again, I've no idea if that is good, but doesn't sound like it.

Deviance: A measure of model fit, comparing the fit of the model to a saturated model (perfect fit). Again a meaningless value obtained. This is peak science.

Degrees of Freedom Residual: The degrees of freedom associated with the residuals.

Fixed Effects:

The estimated baseline "bprs" score when "week" and "treatment" are zero.

week: The estimated change in "bprs" for a one-unit increase in "week."
The negative estimate (-2.2704) suggests a decrease in "bprs" over time.

treatment2:
The estimated difference in baseline "bprs" between treatment group 2 and the reference group.
The t-value (0.550) suggests that this difference is not statistically significant.

Random Effects:
subject (Intercept):
Variability in baseline "bprs" scores across subjects.
subject (week):
Variability in the effect of "week" on "bprs" across subjects.
**The negative correlation (-0.51) suggests that subjects with higher intercepts tend to have steeper slopes for the effect of time.**

The model suggests that, **on average, "bprs" scores decrease over time.**
The variability in baseline scores and the effect of "week" on "bprs" across subjects is captured by random intercepts and slopes. The estimate for "treatment2" is not statistically significant, suggesting **no significant difference in baseline "bprs" scores between treatment group 2 and the reference group.**


Let's add another reference point. The new formula includes main effects for "week" and "treatment," as well as their interaction over (week * treatment). In other words, how the two different groups vary over time. 
Random effects include a random intercept and slope for "week" within the grouping variable "subject." In other words, variation on individual level.

```{r}
#Another reference

psych_ref2 <- lmer(bprs ~ week * treatment + (1 + week | subject), data = BPRSL)


# print a summary of the model

summary(psych_ref2)

# perform an ANOVA test on the two models
anova(psych_ref2, psych_ref1)

# draw the plot of RATSL with the observed Weight values
ggplot(BPRSL, aes(x = week, y = bprs, linetype = subject, color = treatment)) +
  geom_line() +
  scale_x_continuous(name = "Time (weeks)") +
  scale_y_continuous(name = "Health stress index points") +
  theme(legend.position = "bottom")



# Create a vector of the fitted values
Fitted <- fitted(psych_ref2)

# Create a new column fitted to BPRSL

BPRSL <- BPRSL %>%
  mutate(Fitted = Fitted)

# draw the plot with the Fitted values

# draw the plot of RATSL with the observed Weight values
ggplot(BPRSL, aes(x = week, y = Fitted, linetype = subject, color = treatment)) +
  geom_line() +
  scale_x_continuous(name = "Time (weeks)") +
  scale_y_continuous(name = "Health stress index points") +
  theme(legend.position = "bottom")




```
<br>
The model with the interaction term is compared to the previous model without the interaction term using a likelihood ratio test. The Chi-squared test statistic is 3.1712 with 1 degree of freedom, and the p-value is 0.07495.
The p-value suggests that adding the interaction term may or may not significantly improve the fit, depending on the chosen significance level. 

The model includes main effects and an interaction term between time and treatment groups.
Random effects capture variability across subjects in baseline scores and the effect of time.
The likelihood ratio test indicates a potential improvement in model fit with the inclusion of the interaction term, though the significance is borderline.

In other words, it's about as reliable and conclusive as mental health and mental health treatment. In that sense, we can now rely on the fitted model that clearly demonstrate, that if you receive high numbers on mental health stressor query, no matter what treatment, the situation is likely to to improve over time - although your results may vary. 

It's another great day for social sciences.

